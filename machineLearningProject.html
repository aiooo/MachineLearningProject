<!DOCTYPE html><html><head><meta charset="utf-8"><title>machineLearningProject</title><style></style></head><body>
<h1 id="practical-machine-learning-project-peer-assessment">Practical Machine Learning Project - Peer Assessment</h1>
<h2 id="overview">Overview</h2>
<p>The goal of the project is  to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants and predict the correct /incorrect way of performing barbell lifts.
Tha algorithm for prediction has been created basing on randomForest function from the &#39;caret&#39; package. The results allow us to predict the performance basing on selected 53 variables.</p>
<h2 id="getting-and-cleaning-data">Getting and cleaning data</h2>
<p>The first step was to read the data and libraries needed for performing the prediction:</p>
<pre><code class="lang-r">
<span class="hljs-keyword">library</span>(caret)
<span class="hljs-keyword">library</span>(randomForest)
</code></pre>
<pre><code>## Loading required package: lattice
## Loading required package: ggplot2
## Loading required package: methods
</code></pre><pre><code class="lang-r"><span class="hljs-keyword">library</span>(randomForest)
</code></pre>
<pre><code>## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.
</code></pre><pre><code class="lang-r">
setwd(<span class="hljs-string">"/Users/barbara/Data"</span>)
trainingData&lt;-read.csv(<span class="hljs-string">"pml-training.csv"</span>)
dim(trainingData)
testData&lt;-read.csv(<span class="hljs-string">"pml-testing.csv"</span>)
</code></pre>
<pre><code>## [1] 19622   160
</code></pre><pre><code class="lang-r">testData&lt;-read.csv(<span class="hljs-string">"pml-testing.csv"</span>)
dim(testData)
</code></pre>
<pre><code>## [1]  20 160
</code></pre><p>Cleaning data included changing factor into numeric values, skipping irrelevant data and data with high amount of NA. The result is a data frame with 19622 rows and 54 columns (including &#39;classe&#39; column). Since there are zero and negative values in the data, logarithms cannot be calculated.</p>
<pre><code class="lang-r">trainingData2&lt;-trainingData[,colSums(is.na(trainingData)) &lt; <span class="hljs-number">1000</span>]

trainingData2&lt;-trainingData2[,c(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">7</span>:<span class="hljs-number">11</span>, <span class="hljs-number">21</span>:<span class="hljs-number">42</span>, <span class="hljs-number">49</span>:<span class="hljs-number">51</span>, <span class="hljs-number">61</span>:<span class="hljs-number">72</span>, <span class="hljs-number">84</span>:<span class="hljs-number">93</span>)]
</code></pre>
<p>Finally the dataset was split into training and validating (&#39;testing&#39;) set.</p>
<pre><code class="lang-r">inTrain&lt;-createDataPartition(trainingData2[,<span class="hljs-number">1</span>], p=<span class="hljs-number">0.75</span>, list=<span class="hljs-literal">FALSE</span>)
training&lt;-trainingData2[ inTrain,]
testing&lt;-trainingData2[-inTrain,]
</code></pre>
<h2 id="exploratory-analysis">Exploratory analysis</h2>
<p>Plotting the one-by-one relations showed that it&#39;s impossible to find a few major coefficients influencing the result.
The correlation analysis showed that many of the coefficients are correlated (ie. gyros_arm_y, gyros_arm_x; magnet_arm_x, accel_arm_x; magnet_arm_z, magnet_arm_y). 
This indicates that data need multifactor analysis algorithm, such as random forest, or, possibly, processing.</p>
<pre><code class="lang-r">plot(trainingData2[,<span class="hljs-number">1</span>]~., trainingData2)

corr&lt;-abs(cor(trainingData2[,-<span class="hljs-number">54</span>]))
diag(corr)&lt;-<span class="hljs-number">0</span>
which(corr&gt;<span class="hljs-number">0.8</span>, arr.ind=<span class="hljs-literal">TRUE</span>)
</code></pre>
<h2 id="training-and-validation">Training and validation</h2>
<p>Two random forest analyses, performed both with and without PCA preprocessing, revealed  very similar results. Below I present un-preprocessed data, which did a little better in accuracy and Kappa testing. </p>
<pre><code class="lang-r">fitForest&lt;-train(training$classe~., data=training, method=<span class="hljs-string">"rf"</span>, prox=<span class="hljs-literal">TRUE</span>)
</code></pre>
<pre><code class="lang-r">fitForest
</code></pre>
<pre><code>## Random Forest 

14718 samples
   53 predictor
    5 classes: &#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39; 

No pre-processing
Resampling: Bootstrapped (25 reps) 

Summary of sample sizes: 14718, 14718, 14718, 14718, 14718, 14718, ... 

Resampling results across tuning parameters:

  mtry  Accuracy  Kappa  Accuracy SD  Kappa SD
   2    0.995     0.993  0.001520     0.001925
  27    0.998     0.997  0.000737     0.000934
  53    0.995     0.994  0.002409     0.003053

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 27.
</code></pre><pre><code class="lang-r">fitForest$finalModel
</code></pre>
<pre><code>##  Call:
 randomForest(x = x, y = y, mtry = param$mtry, proximity = TRUE) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 27

        OOB estimate of  error rate: 0.11%
Confusion matrix:
     A    B    C    D    E  class.error
A 4230    0    0    0    0 0.0000000000
B    4 2832    2    1    0 0.0024656569
C    0    3 2591    1    0 0.0015414258
D    0    0    3 2354    0 0.0012728044
E    0    0    0    2 2695 0.0007415647
</code></pre><p>Validation has been performed on the pprepared &#39;testing&#39; dataset. The result of prediction, compared with the &#39;true&#39; data, shows very high accuracy, sensivity and specificity of the model.</p>
<pre><code class="lang-r">pred&lt;-predict(fitForest, testing)
confusionMatrix(pred, testing[,<span class="hljs-number">1</span>])
</code></pre>
<h2 id="testing">Testing</h2>
<p>To test the model on a test dataset, the data had to be prepared (the number of columns reduced in accordance with the training dataset). The final data frame consists of 53 columns (all the columns from training dataset except for &#39;classe&#39; data).</p>
<pre><code class="lang-r">testData2&lt;-testData[intersect(colnames(trainingData2),colnames(testData))]
dim(testData2)
</code></pre>
<p>The following code served for generating the predictions for the automatic submission.</p>
<pre><code class="lang-r">predictions &lt;- predict(fitForest,testData2)
</code></pre>

</body></html>